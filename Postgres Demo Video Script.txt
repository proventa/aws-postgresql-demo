-- Title --
Hallo zusammen,
Heute möchten wir euch zeigen, wie man einen containerisierten, hochverfügbaren Postgres Cluster provisionieren kann.

-- Einleitung --
Wir haben uns eine Frage gestellt: warum sollten wir überhaupt unsere Architektur als Proof of Concept entwickeln wollen?

Es gibt doch Kubernetes. Warum nutzen wir es nicht einfach?

K8s "machen" ist komplizierter als gedacht.

1. Erstmal muss sicher gestellt sein, dass es mindestens ein Team gibt, dass sich mit K8s gut auskennt. K8s zu erlernen bewirkt eine steile steile Lernkurve, die viel Zeit in Anspruch nehmen kann.
2. Kubernetes fordert recht hohe Mindestanforderungen für den produktiven Einsatz.
3. Wenn hochsensible Daten verarbeitet werden, kann auch nicht auf Managed Service eines Cloud Service Provider wie z.B. AWS zurück gegriffen werden.

Am Ende muss man sich überlegen, ob die Investition in K8s sich lohnt.

Wenn dies nicht der Fall ist, dann designen wir unseren eigenen Lightweight Postgres HA-Cluster, der auch ein paar Features von K8s hat, wie z.B.:

1. Hochverfügbarkeit
2. Wartungsarm, weil alles automatisiert ist
3. Rolling Updates

-- High-Level Architecture --
Jetzt steigen wir in die Architektur ein.
Wir brauchen 4 Komponenten, nämlich:
1. Einen etcd-Cluster

2. Der Postgres Cluster. Da werden wir das Spilo Image von Zalando nutzen.

3. Dann haben wir einen S3 Bucket, wo die Backups der Datenbanken gespeichert werden.

4. Und am Ende der Monitoring-Cluster.

-- Ansible --
Wir werden alle Ressourcen, die wir benötigen, über Ansible provisionieren. Wir verwenden die AWS EC2 Instanzen für die etcd, Postgres/Patroni sowie Grafana/Prometheus und ein S3 Bucket für die WAL-E/WAL-G Backups.

-- Fedora CoreOS --
Bei allen EC2 Instanzen wird Fedora CoreOS als Betriebssystem verwendet.

Bei Fedora CoreOS gibt es die sogenannte Butane-Datei, die im YAML formatiert ist. Da kann man vieles konfigurieren, wie z.B. die Users, Dateien, oder Systemd  Unit Files.

Danach wird die Butane-Datei in eine JSON formatierte Ignition-Datei konvertiert.

Am Ende kann die Ignition-Datei als UserData beim Provisionieren der EC2 Machine übergeben werden, sodass die Konfiguration beim BootUp wahrgenommen wird


-- etcd Playbook --
Als Beispiel haben wir hier einen Task zum Provisionieren einer EC2 Machine. Wie wir sehen können, gibt es viele Konfigurationen, wie die Anzahl der Maschinen, Tags, Region, etc. Was uns aber momentan interessiert ist der Parameter UserData, dem wir die Ignition-Datei anhängen. Also, lasst uns die Butane-Datei anschauen!

-- etcd Butane --
Hier ist die Systemd Unit File zum etcd.

Wir verwenden Podman und das offizielle etcd Image.

Der etcd Service erhält die komplette Konfiguration als Parameter, wie z.B. die TLS-Zertifikate, oder den DNS-Namen zum Discovery Service.

Das wäre es für den etcd Service. Jetzt schauen wir uns das Spilo Image an.

-- Patroni Playbook --
Als Bestandteil der Hochverfügbarkeit für PostgreSQL verwenden wir einen AWS Load Balancer. Diese Aufgabe kann natürlich auch bspw. von HAProxy übernommen werden.

-- Patroni Butane --
So, hier haben wir die Systemd Unit File zu dem Spilo Image.

Und wie man sieht, gibt es vieles, was man konfigurieren kann, wie z.B. die ETCD Hosts, REST API SSL, und einen S3 Bucket für das Backup.

Weitere Parameter können bei Bedarf für das Spilo Image konfiguriert werden, wir schauen uns jetzt aber erst mal Prometheus und Grafana an!

-- Monitoring Butane --

Für Demo Zwecke haben wir für die Konfiguration von Grafana und Prometheus minimal gehalten.

Das heißt, für Prometheus haben wir einfach die Config-Datei gemapped und für Grafana nur das Admin Password übergeben.

Und somit haben wir alle Konfigurationsdateien angeschaut und können die EC2 Instanzen provisionieren!


-- After Provisioning -- 
Nachdem wir alle Playbooks ausgeführt haben, können wir die EC2 Instanzen in der AWS Konsole anschauen.

Hier haben wir insgesamt 7 Instanzen. 3 davon sind für den etcd Cluster, die andere 3 für Spilo bzw. den Postgres Cluster, und die letzte ist dann für den Monitoring Cluster.

Außerdem haben wir auch einen Load Balancer für den Postgres Cluster.

-- Testing PG --

Lasst uns den DNS-Namen von dem Load Balancer kopieren und versuchen auf die Datenbank zu zugreifen.

Wir können dafür die PSQL CLI verwenden.

So, hier ist der Load Balancer DNS-Name, und dann in diesem Fall nutzen wir den Port 6432 und den User Postgres.

Dann geben wir das Password ein, und schon sind wir mit der Datenbank verbunden. 

-- Terminate Leader Node --
Was wir jetzt als nächstes machen können, ist einen Failover zu simulieren. Wir terminieren den Leader Node, und dann versuchen wir wieder eine Verbindung mit der Datenbank herzustellen.

** Maschine Terminiert **

Ok. der Leader ist terminiert, und jetzt können wir versuchen, wieder auf die Datenbank zu zugreifen.

Also, benutzen wir den gleichen Befehl mit denselben Parametern wie DNS-Name, Port und User.

Und jetzt sehen wir, dass die Datenbank noch verfügbar ist.

Das hat funktioniert, weil Patroni erkennt, dass der Leader Node terminiert wurde, und somit ein neuer Leader Node gewählt werden muss. Danach können Anfragen von dem neuen Primary Node verarbeitet werden.
 
-- Dashboards --
Was wir noch machen können, ist dass wir den Status des Clusters auf Grafana anschauen. 

Dazu besuchen wir das Grafana UI mit der IP Adresse von der Monitoring Maschine.

Hier haben wir verschiedene Dashboards wie das etcd Dashboard, PgBouncer Dashboard, Postgres Dashboard und das Patroni Dashboard.

-- Closing --
Okay, wir haben einen ersten Überblick erhalten. Jetzt gibt es weitere Fragen: z.B wie Betriebssystem-Updates so verwaltet werden können, dass nicht alle Nodes gleichzeitig ein Update machen, was zu Downtime führen würde, sondern ein Node nach dem anderen.

Da kommt Airlock ins Spiel. 

Wollt ihr wissen, wie das funktioniert? Bleibt dran für unser nächstes Video!

Vielen Dank für das Zuschauen! Und bis zum nächstes Mal!